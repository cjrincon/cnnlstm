% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/hp_tuning_LSTM.R
\name{hp_tuning_LSTM}
\alias{hp_tuning_LSTM}
\title{Define the hyperparameters of the long-short term memory LSTM}
\usage{
hp_tuning_LSTM(
  data,
  train_size = 0.8,
  random_seed = 1234,
  n_steps_LSTM,
  f_neuron,
  f_drop,
  s_neuron,
  s_drop,
  learn_rate,
  epoc
)
}
\arguments{
\item{data}{Dataset to split and tuning}

\item{train_size}{Proportion of the x to split as train subset. By default the value is 0.8}

\item{random_seed}{An integer to control the random number generator used. By default the value is 1234}

\item{n_steps_LSTM}{Number of steps to split into samples}

\item{f_neuron}{Number of neuron in the first LSTM layer}

\item{f_drop}{Dropout rate in the first LSTM layer}

\item{s_neuron}{Number of neuron in the second LSTM layer}

\item{s_drop}{Dropout rate in the second LSTM layer}

\item{learn_rate}{Learning rate in the compilation with Adam optimizer}

\item{epoc}{Number of epochs in the training model}
}
\value{
Returns:
Dataframe with the combinations of the parameters and the calculated error
Better combination of hyperparameters that minimizes the train and test error
}
\description{
Define the hyperparameters of the long-short term memory LSTM
}
\author{
Catherine Rincon, \email{catherine.rincon@udea.edu.co}
}
